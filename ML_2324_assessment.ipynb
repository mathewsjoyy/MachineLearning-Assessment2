{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1308ea1-b9a2-4f7a-b000-f41d0b38aff2",
   "metadata": {},
   "source": [
    "# Machine Learning (CMP3751M/CMP9772M) - Assessment 02\n",
    "\n",
    "Through the following notebook, you will be analysing a dataset and fitting a classification model to this dataset.\n",
    "\n",
    "The assessment is structured as follows:\n",
    "- [Dataset description](#Dataset-description)\n",
    "- [Loading the dataset](#Loading-the-dataset)\n",
    "- [Simple classification model](#Simple-classification-model)\n",
    "    - [Creating a training and testing set](#Creating-a-training-and-testing-set)\n",
    "    - [Training a classifier](#Training-a-classifier)\n",
    "- [Improved evaluation strategy](#Improved-evaluation-strategy)\n",
    "- [Different models and parameter search](#Different-models-and-parameter-search)\n",
    "- [Ensembles](#Ensembles)\n",
    "- [Final model evaluation](#Final-model-evaluation)\n",
    "- [References](#References)\n",
    "\n",
    "**Notes:**\n",
    "- The (%) noted above are out of 100; this will be scaled down to **maximum of 60 marks** for the assessment **(or maximum of 50 marks for CMP9772M)** .\n",
    "- Any discussion not supported by your implementation will not be awarded marks.\n",
    "- **Do not modify** and code provided as a **TESTING CELL**.\n",
    "- Make sure to **fix all the random seeds** in any parts of your solution, so it can be reproduced exactly.\n",
    "- The notebook, as provided, runs without errors (without solving the assessment). Make sure that the solution, or the partial solution, you hand in, also **runs without errors** on the data provided. If you have a partial solution causing errors which you would like to show, please include it as a comment.\n",
    "- Take care to include references to any external sources used. Check the [References](#References) section, the below cell, and the exambles through the assessment text for examples of how to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54719c9-3bc8-47c1-b620-831b73083ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to reference your sources! Check the bottom of the file, and examples used in the text of the assessment,\n",
    "# for including references to papers and software in your textual answers\n",
    "\n",
    "# Also add a reference in your solution cell before defining a class/function/method, eg.:\n",
    "\n",
    "# This code is a modified and extended version of [2]\n",
    "# OR\n",
    "# This code is a modified and extended version of https://stackoverflow.com/q/522563/884412\n",
    "##############\n",
    "## THE CODE ##\n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af12af-dd93-44ba-a161-dbab8f2017c0",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "The the assessment will be done on the dataset containing only numerical features describing the physical and chemical properties of the Li-ion battery, which can be classified on the basis of their crystal system [1]. (The dataset for this assessment has been adapted from the full dataset which can be found [here](https://www.kaggle.com/datasets/divyansh22/crystal-system-properties-for-liion-batteries), shared in the public domain by Divyansh Agrawal).\n",
    "\n",
    "Each sample corresponds to the properties of a battery, and consists of following features:\n",
    "\n",
    "| Feature Name      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Formation Energy`       | `float`: eV | Formation energy of the material. |\n",
    "| `E Above Hull` | `float`: eV | Energy of decomposition of material into most stable ones. |\n",
    "| `Band Gap` | `float`: eV | Band gap. |\n",
    "| `Nsites` | `int`: count | Number of atoms in the unit cell of the crystal. |\n",
    "| `Density` | `float`: gm/cc | The density of bulk crystalline materials. |\n",
    "| `Volume` | `float` | The unit cell volume of the material. |\n",
    "\n",
    "The goal for the assessment is to predict whether the crystal system of the battery is _monoclinic_, _orthorhombic_ or _triclinic_, which provides a classification for each sample:\n",
    "\n",
    "| Class      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Crystal System`  | `string`: class designation | Class of the crystal system. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3ee2b-8c56-49f4-a5da-bbc7330283cb",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "The dataset is given in _batteries.csv_ file provided on Blackboard. **Load the dataset into two [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html)s.**: \n",
    "- The variable `X` should be a 2D [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) containing all the samples and their features from the dataset, one sample per row. \n",
    "- The variable `y` should be a 1D [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) containing the ground truth (class) as given in the `'Crystal System'` field of the _.csv_ file.\n",
    "- _Note_: The class in the `'Crystal System'` column is given as a string. Make sure you encode the class as an integer number in your ground truth `y`.\n",
    "- _Note_: You should make sure that your code for loading the dataset is guided by the information about the dataset, and the dataset description you provide as your answer.\n",
    "\n",
    "**Describe the dataset**. Provide a basic description of the dataset. How many samples are there in the dataset? How many distinct classes? What types of features describe the samples in the dataset? Are there any missing values in the dataset? (Make sure these are properly handled). \n",
    "- _Note_: Make sure all your answers are supported by your implementation. Answers not supported by your implementation will not score any marks.\n",
    "\n",
    "Provide your code to _load the dataset_ and the code that will allow you to _describe the dataset_ in the **SOLUTION CELL**. Provide your description of the dataset in the **ANSWER CELL**. A correct solution should result in no errors when running the **TESTING CELL** provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc2b74-5ce4-47ba-815a-138e893c599c",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae8e88a2-d3c4-4b7a-b8b9-b679fa6f6b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The dataset contains 339 samples and 7 features.\n",
      "\n",
      "2. The dataset has 3 distinct classes.\n",
      "\n",
      "3. The features consist of Formation Energy, E Above Hull, Band Gap, Nsites, Density, Volume.\n",
      "\n",
      "4. The features are of type:\n",
      "Formation Energy    float64\n",
      "E Above Hull        float64\n",
      "Band Gap            float64\n",
      "Nsites                int64\n",
      "Density             float64\n",
      "Volume              float64\n",
      "Crystal System       object\n",
      "dtype: object\n",
      "\n",
      "5. Columns containing missing values (with count):\n",
      "Formation Energy    1\n",
      "E Above Hull        0\n",
      "Band Gap            2\n",
      "Nsites              0\n",
      "Density             2\n",
      "Volume              0\n",
      "Crystal System      0\n",
      "dtype: int64\n",
      "6. The number of outliers found is 34.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "\n",
    "# Load the battery.csv data set into a pandas DataFrame\n",
    "df = pd.read_csv('batteries.csv')\n",
    "\n",
    "# Describe data set\n",
    "print(f\"1. The dataset contains {df.shape[0]} samples and {df.shape[1]} features.\\n\")\n",
    "print(f\"2. The dataset has {len(df['Crystal System'].unique())} distinct classes.\\n\")\n",
    "print(f\"3. The features consist of {', '.join(df.columns[:-1])}.\\n\")\n",
    "print(f\"4. The features are of type:\\n{df.dtypes}\\n\")\n",
    "print(f\"5. Columns containing missing values (with count):\\n{df.isnull().sum()}\")\n",
    "\n",
    "# Load all samples and their features from the data frame into variable X\n",
    "X = df.loc[:, df.columns!='Crystal System'].values\n",
    "\n",
    "# Load the ground truth labels from the data frame into variable y\n",
    "y = df[\"Crystal System\"].values\n",
    "\n",
    "# Encode the labels as integers (REFERENCE THIS E.G. SKLEARN WEBSITE DOCS)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Create an instance of the SimpleImputer class\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer to the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# Transform the dataset using the imputer\n",
    "X = imputer.transform(X)\n",
    "\n",
    "\n",
    "# Soloution :\n",
    "# use class mapping from workshop 3 to encode the labels as integers\n",
    "#  then use simple inputer to handle missing values\n",
    "# then find outliers using local outlier factor\n",
    "\n",
    "\n",
    "# Find outliers in the data set\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "\n",
    "y_pred = lof.fit_predict(X)\n",
    "\n",
    "# Print the number of outliers found\n",
    "print(f\"6. The number of outliers found is {np.sum(y_pred==-1)}.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae52cc-9f39-4009-b684-cafb1cd35361",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3ed4f9-8d94-41f8-8f12-b18919fbe884",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X.shape) == 2)\n",
    "assert(len(y.shape) == 1)\n",
    "assert(X.shape[0] == y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60db068-206c-4471-8b54-780a653cc315",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4e42d-cc07-4982-91fb-97c7b2f282a1",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0117c-fdca-4d5a-9139-c0addbeb94a2",
   "metadata": {},
   "source": [
    "## Simple classification model\n",
    "\n",
    "To get the feel for the dataset, the first step will be to build train a simple classification model for this dataset. Do this in two steps detailed below:\n",
    "1. Set aside some data for training and for testing.\n",
    "2. Train a simple classifier on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac10b8-eac0-4b64-a82e-40781846c8d1",
   "metadata": {},
   "source": [
    "### Creating a training and testing set\n",
    "\n",
    "**Set aside 20\\% of the data for testing, and use the remaining 80\\% to train your model.** Make sure to fix any random seeds if you use any functions or methods relying on those, so your experiments are _fully repeatable_. Initialise the following variables:\n",
    "- `X_train` should contain the features corresponding to your training data.\n",
    "- `y_train` should contain the ground truth of your training data.\n",
    "- `X_test` should contain the features corresponding to your testing data.\n",
    "- `y_train` should contain the ground truth associated to your testing data.\n",
    "\n",
    "_Note:_ No additional marks will be rewarded for implementing an advanced data splitting strategy on this task. The purpose of this task is to start working with the dataset by applying a simple approach; you will have the chance to implement more complex evaluation pipelines in a later task.\n",
    "\n",
    "Provide your implementation in the **SOLUTION CELL (a)** below. A correct solution should result in no errors when running the **TESTING CELL** provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bc50e-8318-428d-b29b-220b4dadb283",
   "metadata": {},
   "source": [
    "### Training a classifier\n",
    "\n",
    "**Train a simple classifier,** (of your choosing) **with fixed parameters** on the dataset, and **calculate accuracy on the test set**.\n",
    "- Define a function `model_accuracy(y_test, y_pred)` to compare the ground truth given in `y_test` to predictions given in `y_pred` and calculate accuracy.\n",
    "- **Store the model** in the variable named `model`. For the model, you may chose any classifier with which you are familiar (e.g. K Nearest Neighbours), or implement your own classifier. Make sure you **train your model** using the _training data_ only (`X_train`, `y_train`).\n",
    "- Use the model to **predict the classes of the data** in the testing set (`X_test`), and calculate the accuracy by comparing the predictions with the ground truth for the testing set (`y_test`). **Store the predictions** in a variable called `y_test`.\n",
    "\n",
    "_Note:_ Do not implement an advanced strategy to chose the parameters of your classifier here, as that will be a topic of a latter question.\n",
    "\n",
    "_Note:_ If you implement your own classifier, make sure you implement it as a _class_ following the _sklearn_ standard for classifiers (i.e. make sure it implements the `fit(X, y)` method to train the model, and `predict(X)` method to use the trained model to predict the classes of provided samples.\n",
    "\n",
    "\n",
    "**Discuss the advantages and shortcomings** of the evaluation strategy implemented through this task. Discuss both the data split used for evaluation and the choice of metric. Taking into account the information you know about the dataset, what kind of accuracy scores can you expect on this dataset from a good and bad performing model? Based on the information you have so far, comment on the performance of the model you have trained on the provided dataset.\n",
    "\n",
    "Provide your implementation in the **SOLUTION CELL (b)** below. The **TESTING CELL** below should run without errors and will print the prediction of your model for the first sample in the test set, and the accuracy as calculated by your `model_accuracy` function. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d0156-553b-4c89-a594-fa1c88e592b4",
   "metadata": {},
   "source": [
    "**SOLUTION CELL (a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c823edbd-0d8b-4238-9ae7-db683d651b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "\n",
    "# Split the data set into a training set and a test set 80 % / 20 %\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046238d-602f-4ed1-8f41-cd71fdbb853d",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31fc35ed-610c-47bc-acce-0060f0f72034",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_test.shape[0] == y_test.shape[0])\n",
    "assert(X_train.shape[1] == X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af37dd-393c-4278-9972-c10677ad6189",
   "metadata": {},
   "source": [
    "**SOLUTION CELL (b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0e7c2a-5d95-4071-ae09-68c14e00bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(y_test, y_pred):\n",
    "    # Check that the input lists are of the same length\n",
    "    assert(len(y_test) == len(y_pred))\n",
    "    \n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = np.sum(y_test == y_pred) / len(y_test)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier with 3 neighbors\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "    \n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4e3e6-be93-4fd5-9611-ec06c2a4610a",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b1d52b9-7543-4d59-90d6-43ad5cf08229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "0.5147058823529411\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test[0].reshape(1,-1)))\n",
    "print(model_accuracy(y_test, y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae302f-0899-481e-8901-de9efe493da1",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bd068-84e1-4014-8696-02e3abfe8455",
   "metadata": {},
   "source": [
    "The evaluation method performed was accuracy. \"It is a common metric used to measure the overall\n",
    "correctness of modelâ€™s predictions; it quantifies the ratio of correctly predicted instances to the total number of instances in the dataset\". \n",
    "REFERENCE SLIDE 9 WEEK 2 <------\n",
    "\n",
    "\n",
    "The accuracy evaluation strategy implemented through this task has some advantages and shortcomings. One advantage is that it provides a simple and straightforward way to evaluate the performance of a classifier on both the train and test data. However, there are also some shortcomings to this evaluation strategy.One shortcoming is for example, if the classes are imbalanced, accuracy may be misleading, as a classifier that always predicts the majority class will have a high accuracy but may not be useful in practice. Moreover, the choice of metric may not always be appropriate for the problem at hand. For example, in some cases, precision or recall may be more appropriate metrics to use.\n",
    "\n",
    "An 80% / 20% training/test data split is common in machine learning tasks. However, one significant drawback is that the performance of the models can be highly dependent on the specific split of the data. For instance datasets with some infrequent classes or outliers then fitting the model to the data may produce unwanted results. A better evaluation strategy is k-fold cross validation.\n",
    "\"This is a widely used technique in machine learning and statistical modeling to assess the performance and generalization of a model. It helps in estimating how well a model will perform\n",
    "on unseen data, which is crucial for avoiding either overfitting or underfitting\".\n",
    "REFERENCE SLIDE 21 WEEK 2 <------\n",
    "\n",
    "\n",
    "We can also utilize the use of leave-one-out cross validation (LOOCV). In leave-one-out cross validation, instead of using k folds, the data is used to train the model one time for each item in the set, using all other items as a training set and only one item as the testing set. This will give a model performance metric which is more robust to outliers in the data. However due to our model having a fair few samples this may not be the best approach to take, as LOOCV can be computationally expensive and time consuming.\n",
    "\n",
    "Trying to construct a good / bad accuracy score we can get from the model depends on several factors such as the complexity of the problem, the quality of the data, and the choice of classification algorithm. Based on the structure of the dataset, the dataset having 6 features (independent variables), 3 possible classes (dependent variables) and 339 samples, we can expect a good performing model to achieve an accuracy score of around 80% or higher, while a bad performing model would achieve an accuracy score of around 50% or lower (essentially being 50% or lower is saying the model is worse then simply just guessing). Based on the information we have so far, the model we have trained on the provided dataset achieved an accuracy score of 51%. This suggests that the model may not be complex enough to capture the underlying patterns in the data. It is possible that a more complex model or a different classification algorithm may perform better on this dataset. However, it is also important to keep in mind that this is just a preliminary evaluation, and further optimization such as hyperparameter tuning may be possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa832abe-5688-43e2-8d1f-b655b7d4b142",
   "metadata": {},
   "source": [
    "## Improved evaluation strategy\n",
    "\n",
    "After discussing the shortcomings of the simple evaluation strategy used in the previous task, you now have a chance to **propose a better evaluation strategy.** Make sure your chosen strategy **uses all the samples in the dataset** to report the result.\n",
    "- Implement a function `evaluate_model(model, X, y)` to implement your proposed evaluation strategy. The function should evaluate the model given in `model` on the dataset given by `X` with ground truth given by `y`. Note that the function should be passed the _whole of the dataset_ (see **TESTING CELL** below) and should take care of any data splitting internally.\n",
    "- If desired, you may add additional arguments to this function, as long as they have default values and the function runs correctly when called using those default values.\n",
    "- The function should return no values, but instead print the results of the evaluation in a human-readable format.\n",
    "- Include at least one summative metric (providing a single number, e.g. accuracy) and per-class metric (e.g. precision). You are encouraged to select more than one metric of each type.\n",
    "\n",
    "This function will be used to provide a better evaluation of the simple model with fixed parameters used in the previous task.\n",
    "\n",
    "**Discuss your chosen evaluation strategy**, including both the data split and the evaluation metrics. Which data splitting strategy did you chose and why? Which metrics did you chose, and why? Briefly explain the chosen data splitting strategy. What additional information can your additional metrics provide beyond accuracy?\n",
    "\n",
    "Provide your implementation of this function in the **SOLUTION CELL**. You may also include any additional evaluation calls you want to include in this code cell. The **TESTING CELL** will perform a basic evaluation of your `model` using the `evaluate_model` function implemented for this task. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e4c6a-7818-4b68-a7db-14e73dae16b8",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd9c1591-9cfa-4831-9f56-d42557f31897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# This code is a modified and extended version of [3] [4]\n",
    "\"\"\"\n",
    "Function to evaluate a model using cross validation and summative and per class metrics\n",
    "@param model: machine learning model to evaluate\n",
    "@param X: data set to evaluate the model on (features)\n",
    "@param y: ground truth labels for the data set (labels)\n",
    "@return: Printed results of the evaluation\n",
    "\"\"\"\n",
    "def evaluate_model(model, X, y, k=10):\n",
    "    print('Evaluating model...')\n",
    "\n",
    "    # Perform K fold cross validation\n",
    "    kf = KFold(n_splits=k, random_state=0, shuffle=True)\n",
    "    \n",
    "    # Calculate the metrics of the model using cross validation score\n",
    "    f1_scores = cross_val_score(model, X, y, scoring='f1_macro', cv=kf)\n",
    "    precision_scores = cross_val_score(model, X, y, scoring='precision_macro', cv=kf)\n",
    "    recall_scores = cross_val_score(model, X, y, scoring='recall_macro', cv=kf)\n",
    "    accuracy_scores = cross_val_score(model, X, y, scoring='accuracy', cv=kf)\n",
    "    \n",
    "    # Report the mean and standard deviation of the scores (mean across all folds)\n",
    "    print(f\"Evaluation results (Mean Across All Folds ({k})):\")\n",
    "    print(f'F1 score: {round(f1_scores.mean(),3)} (std: {round(f1_scores.std(),3)})')\n",
    "    print(f'Precision: {round(precision_scores.mean(),3)} (std: {round(precision_scores.std(),3)})')\n",
    "    print(f'Recall: {round(recall_scores.mean(),3)} (std: {round(recall_scores.std(),3)})')\n",
    "    print(f'Accuracy: {round(accuracy_scores.mean(),3)} (std: {round(accuracy_scores.std(),3)})')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Show the precision and recall per class (per class metrics)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6d8f2-fae8-4f49-839c-3c145b4573c3",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73a5b7ca-e08d-4bf0-8b0b-2aeb4ee4982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Evaluation results (Mean Across All Folds (10)):\n",
      "F1 score: 0.533 (std: 0.086)\n",
      "Precision: 0.57 (std: 0.093)\n",
      "Recall: 0.544 (std: 0.087)\n",
      "Accuracy: 0.57 (std: 0.075)\n",
      "[0.39444444 0.65151515 0.68496732 0.52380952 0.73280423 0.58333333\n",
      " 0.70968615 0.49607843 0.60582011 0.60561661]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46babc-b9bb-4825-9a24-2f4ef812d493",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6281e9-3b43-41fd-9c67-fd2caf83a071",
   "metadata": {},
   "source": [
    "_Write your answer here._\n",
    "\n",
    "Explain why we compute the mean / average and also the standard devation of the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce4cf3-0f68-4884-bf55-e76eed093144",
   "metadata": {},
   "source": [
    "## Different models and parameter search\n",
    "\n",
    "Now that you have a [better evaluation strategy](#Improved-evaluation-strategy) implemented, it is time to try out different models, and try out different parameter combinations for these models.\n",
    "\n",
    "**Fit at least three different (types of) machine learning models** to the provided dataset. (_Note:_ Make sure at least 2 out of your 3 chosen types have different model parameters which can be adjusted). **Try different parameters for all of your models** (which have parameters). Use a single summative metric of your choice to choose between the different types of models, and the models with different parameters. Finally, **choose thee different models, one of each type** and assign them to variables `model_1`, `model_2` and `model_3`.\n",
    "\n",
    "**Discuss your choice of models, and your procedure to adjust the model parameters**. Discuss how you reached the decision about the best model amongst the models of the same type (which metric was selected, and why). Also discuss any shortcomings of your approach and how (and if) you could improve on this. After evaluating these models on the dataset, **discuss and compare their performance on the provided data.**\n",
    "\n",
    "Implement your solution in the **SOLUTION CELL**. The **TESTING CELL** will evaluate the three best models selected by you, using your evaluation strategy. Discuss your choices in the **ANSWER CELL**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353db6d-39a5-4022-bee1-4b55d5bb4d09",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adfbbe1a-4a42-4681-859b-bc4bf348d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "##### replace these lines ######\n",
    "#### Code  adapted from [2] ####\n",
    "model_1 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "model_2 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "model_3 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "################################\n",
    "# model_1 = ...\n",
    "# model_2 = ...\n",
    "# model_3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a52581-71a4-4c75-bc3a-861fe83e40a4",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2131851-eb53-4b64-8b12-d53af5612328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'estimator' parameter of check_scoring must be an object implementing 'fit'. Got <__main__.DummyClassifier object at 0x000002062BBDDB50> instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mathews\\OneDrive\\Documents\\BSc_Computer_Science_Files\\Year_3\\Machine_Learning\\MathewsJoy_MachineLearning_Assessment2\\ML_2324_assessment.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mathews/OneDrive/Documents/BSc_Computer_Science_Files/Year_3/Machine_Learning/MathewsJoy_MachineLearning_Assessment2/ML_2324_assessment.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate_model(model_1, X, y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mathews/OneDrive/Documents/BSc_Computer_Science_Files/Year_3/Machine_Learning/MathewsJoy_MachineLearning_Assessment2/ML_2324_assessment.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mathews/OneDrive/Documents/BSc_Computer_Science_Files/Year_3/Machine_Learning/MathewsJoy_MachineLearning_Assessment2/ML_2324_assessment.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m evaluate_model(model_2, X, y)\n",
      "\u001b[1;32mc:\\Users\\Mathews\\OneDrive\\Documents\\BSc_Computer_Science_Files\\Year_3\\Machine_Learning\\MathewsJoy_MachineLearning_Assessment2\\ML_2324_assessment.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mathews/OneDrive/Documents/BSc_Computer_Science_Files/Year_3/Machine_Learning/MathewsJoy_MachineLearning_Assessment2/ML_2324_assessment.ipynb#X46sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m kf \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39mk, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mathews/OneDrive/Documents/BSc_Computer_Science_Files/Year_3/Machine_Learning/MathewsJoy_MachineLearning_Assessment2/ML_2324_assessment.ipynb#X46sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Calculate the metrics of the model using cross validation score\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Mathews/OneDrive/Documents/BSc_Computer_Science_Files/Year_3/Machine_Learning/MathewsJoy_MachineLearning_Assessment2/ML_2324_assessment.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m f1_scores \u001b[39m=\u001b[39m cross_val_score(model, X, y, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mf1_macro\u001b[39;49m\u001b[39m'\u001b[39;49m, cv\u001b[39m=\u001b[39;49mkf)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mathews/OneDrive/Documents/BSc_Computer_Science_Files/Year_3/Machine_Learning/MathewsJoy_MachineLearning_Assessment2/ML_2324_assessment.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m precision_scores \u001b[39m=\u001b[39m cross_val_score(model, X, y, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprecision_macro\u001b[39m\u001b[39m'\u001b[39m, cv\u001b[39m=\u001b[39mkf)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mathews/OneDrive/Documents/BSc_Computer_Science_Files/Year_3/Machine_Learning/MathewsJoy_MachineLearning_Assessment2/ML_2324_assessment.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m recall_scores \u001b[39m=\u001b[39m cross_val_score(model, X, y, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecall_macro\u001b[39m\u001b[39m'\u001b[39m, cv\u001b[39m=\u001b[39mkf)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:560\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Evaluate a score by cross-validation.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[39mRead more in the :ref:`User Guide <cross_validation>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[39m[0.3315057  0.08022103 0.03531816]\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39;49mscoring)\n\u001b[0;32m    562\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    563\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    564\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     error_score\u001b[39m=\u001b[39merror_score,\n\u001b[0;32m    574\u001b[0m )\n\u001b[0;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:201\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m to_ignore \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    199\u001b[0m params \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 201\u001b[0m validate_parameter_constraints(\n\u001b[0;32m    202\u001b[0m     parameter_constraints, params, caller_name\u001b[39m=\u001b[39;49mfunc\u001b[39m.\u001b[39;49m\u001b[39m__qualname__\u001b[39;49m\n\u001b[0;32m    203\u001b[0m )\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'estimator' parameter of check_scoring must be an object implementing 'fit'. Got <__main__.DummyClassifier object at 0x000002062BBDDB50> instead."
     ]
    }
   ],
   "source": [
    "evaluate_model(model_1, X, y)\n",
    "print()\n",
    "evaluate_model(model_2, X, y)\n",
    "print()\n",
    "evaluate_model(model_3, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ae54c-06a2-4f7e-8fee-80ba278be269",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6439d8-ec84-42d3-8a75-dda5217e871f",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593d5af-582d-47ca-a341-c83bf318bfeb",
   "metadata": {},
   "source": [
    "## Ensembles\n",
    "\n",
    "Sometimes, combining different weak classification models can improve the overall performance of the model. **Implement bagging** for each of your three classification models (`model_1`, `model_2`, `model_3`) [from the previous task](#Different-models-and-parameter-search). Store your models performing bagging over your based models calculated in the previous task in variables called `bagged_1`, `bagged_2` and `bagged_3`. Provide your implementation, running any additional evaluation needed, in the **SOLUTION CELL**\n",
    "\n",
    "The **TESTING CELL** will evaluate your 3 bagged models using your own evaluation procedure. It will also make a voting ensemble consisting of your three base models (`model_1`, `model_2`, `model_3`) and another one made of your bagged models (`bagged_1`, `bagged_2` and `bagged_3`), and evaluate these three voting ensembles.\n",
    "\n",
    "**Discuss** the effect on bagging on your base models. Discuss how you chose the bagging parameters, and justify your choice. Discuss the effect using the voting ensemble had on your model performance. Compare the effect of a voting ensemble on the ensemble models to the effect on the base models. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b14274-bfc0-45fc-87e5-96eb7d120a7e",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8899c-d7fe-4518-bf82-d66442466bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "##### replace these lines ######\n",
    "#### Code  adapted from [2] ####\n",
    "bagged_1 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "bagged_2 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "bagged_3 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "################################\n",
    "# bagged_1 = ...\n",
    "# bagged_2 = ...\n",
    "# bagged_3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e6e22-4eb4-4c71-9c56-2043507671d4",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08fc7e-899b-4511-8075-d816f652c68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "Evaluating model...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "eclf  = VotingClassifier(estimators=[('CLF1', model_1), ('CLF2', model_2), ('CLF3', model_3)], voting='hard')\n",
    "ebclf  = VotingClassifier(estimators=[('BCLF1', bagged_1), ('BCLF2', bagged_2), ('BCLF3', bagged_3)], voting='hard')\n",
    "\n",
    "evaluate_model(bagged_1, X, y)\n",
    "print()\n",
    "evaluate_model(bagged_2, X, y)\n",
    "print()\n",
    "evaluate_model(bagged_3, X, y)\n",
    "print()\n",
    "evaluate_model(eclf, X, y)\n",
    "print()\n",
    "evaluate_model(ebclf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17597da9-0556-4dec-992e-c14e0b9b618a",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e3a0e-f242-404d-a980-e8e9cc928b70",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4b762-a3ca-46ca-b7fa-6f5d5a58902b",
   "metadata": {},
   "source": [
    "## Final model evaluation\n",
    "\n",
    "Based on all the experiments performed for this assessment, **choose a single best model, evaluate it** with [your evaluation procedure](#Improved-evaluation-strategy) and also **display the confusion matrix**. **Discuss the performance achieved by this model**.\n",
    "\n",
    "**You should attempt this cell even if you have not successfully trained all the models required in this assessment, and comment on the best model which _you_ have obtanied.**\n",
    "\n",
    "Implement your solution in the **SOLUTION CELL** below. Add your discussion to the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ead24-194d-4953-adea-a0fe59caa586",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739096d-09a1-4d58-82ef-7c2822727451",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "##### replace these lines ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d736e-8e98-4bf4-8abe-537fbd297998",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3562ce-0610-4055-8be5-b4c23103f245",
   "metadata": {},
   "source": [
    "Finally we achieve an accuracy score of 63\\% which is much improved over the majority-class classifier at 41\\%. The MCC and Kappa metrics confirm that we have now constructed a much better classifier, with scores of around 0.43. The confusion matrix however still shows that the minority class is underfperforming with respect to the other two.:\n",
    "- Class 0 is performing the best, however is much more often misclassified as class 1 (35) than as class 2 (14).\n",
    "- Class 1 is performing second best, and is equally more as likely to be misclassified as class 0.\n",
    "- Class 2 is performing the worst, as it is the minority class. It is quite often confused with class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d346a-6c57-496b-8e0d-d2bf6d51485b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Divyansh Agrawal: Crystal System Properties for Li-ion batteries (dataset) https://www.kaggle.com/datasets/divyansh22/crystal-system-properties-for-liion-batteries/discussion (accessed 28/08/2023)\n",
    "\n",
    "[2] Mateen Ulhaq, Mike Hordecki (code) https://stackoverflow.com/a/522578/884412 (accessed 24/08/2023)\n",
    "\n",
    "[3] https://www.askpython.com/python/examples/k-fold-cross-validation\n",
    "\n",
    "[4] https://stackoverflow.com/questions/46598301/how-to-compute-precision-recall-and-f1-score-of-an-imbalanced-dataset-for-k-fold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
